# -*- coding: utf-8 -*-
"""feature-engineering-mrk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n7nBqO6y2qfH6bHLi9DJQcj39agOvItV
"""

def process_feature_engineering(train_file_path, test_file_path):
    """
    Main function to process feature engineering for Pokémon battle prediction.

    Args:
        train_file_path: Path to training data JSON file
        test_file_path: Path to test data JSON file

    Returns:
        tuple: (train_df, test_df, train_df_raw, test_df_raw, POKEMON_STATS, POKEMON_HP_STATS, pokemon_avg_damage)
    """
    from tqdm.notebook import tqdm
    import numpy as np
    import pandas as pd
    import json
    from typing import List, Dict, Any, Tuple
    from collections import Counter, defaultdict

    # Load train_data
    train_data = []
    with open(train_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            train_data.append(json.loads(line))

    # ---------------------------------------------
    # Global stats built on train_data
    # ---------------------------------------------
    POKEMON_STATS    = build_pokemon_win_stats(train_data, alpha=1.0)
    POKEMON_HP_STATS = build_pokemon_hp_stats(train_data)
    pokemon_avg_damage = build_pokemon_avg_damage(train_data)

    # ---------------------------------------------
    # Run feature extraction
    # ---------------------------------------------
    print("Processing training data...")
    train_df = create_simple_features(train_data)

    print("\nProcessing test data...")
    test_data = []
    with open(test_file_path, 'r', encoding='utf-8') as f:
        for line in f:
            test_data.append(json.loads(line))
    test_df = create_simple_features(test_data)

    print("\nTraining features preview:")

    # --- Manual interactions (robust to missing columns) ---
    def _maybe_add_interactions(df: pd.DataFrame) -> pd.DataFrame:
        def safe_mul(a, b, name):
            if a in df.columns and b in df.columns:
                df[name] = df[a] * df[b]

        # Team strength × move power (full)
        safe_mul("p1_team_stat_avg", "mv_p1_power_mean", "ix_p1avg_x_p1pow")
        # Speed × priority advantage (first 5 turns if available)
        if "spe_max_adv" in df.columns and "mv_priority_count_diff_5" in df.columns:
            df["ix_speed_x_prio5"] = df["spe_max_adv"] * df["mv_priority_count_diff_5"]
        # HP momentum × fraction of advantaged turns
        safe_mul("tl_hp_diff_mean", "tl_frac_turns_advantage", "ix_hpmean_x_fracadv")
        # Early momentum × priority diff (first 5)
        if "early_hp_diff_mean_3" in df.columns and "mv_priority_count_diff_5" in df.columns:
            df["ix_early3_x_prio5"] = df["early_hp_diff_mean_3"] * df["mv_priority_count_diff_5"]
        # STAB advantage × early KO score
        if "stab_stab_ratio_diff_full" in df.columns and "early_first_ko_score_3" in df.columns:
            df["ix_stabdiff_x_firstko"] = df["stab_stab_ratio_diff_full"] * df["early_first_ko_score_3"]
        # Type effectiveness × STAB (full)
        if "ter_p1_vs_p2lead_full" in df.columns and "stab_stab_ratio_diff_full" in df.columns:
            df["ix_ter_x_stab_full"] = df["ter_p1_vs_p2lead_full"] * df["stab_stab_ratio_diff_full"]
        # Type effectiveness × early momentum (first 3)
        if "ter_p1_vs_p2lead_5" in df.columns and "early_hp_diff_mean_3" in df.columns:
            df["ix_ter5_x_early3"] = df["ter_p1_vs_p2lead_5"] * df["early_hp_diff_mean_3"]
        # Lead matchup × early momentum
        if "lead_matchup_p1_index_5" in df.columns and "early_hp_diff_mean_3" in df.columns:
            df["ix_leadmatch5_x_early3"] = df["lead_matchup_p1_index_5"] * df["early_hp_diff_mean_3"]
        # Hazards advantage × priority pressure
        if "hazard_flag_diff" in df.columns and "mv_priority_count_diff_5" in df.columns:
            df["ix_hazards_x_prio5"] = df["hazard_flag_diff"] * df["mv_priority_count_diff_5"]
        return df

    train_df = _maybe_add_interactions(train_df)
    test_df  = _maybe_add_interactions(test_df)

    # Keep raw copies
    train_df_raw = train_df.copy()
    test_df_raw  = test_df.copy()

    # ---- No scaling here (we'll scale inside the Pipeline in Cell 3.2) ----
    # 1) Make sure features are numeric, to avoid casting issues downstream
    num_cols = [c for c in train_df.columns if c not in ("battle_id", "player_won")]
    train_df[num_cols] = train_df[num_cols].apply(pd.to_numeric, errors="coerce").astype("float32")
    test_df[num_cols]  = test_df[num_cols].apply(pd.to_numeric, errors="coerce").astype("float32")

    # 2) Replace inf/-inf with NaN (safer for imputers)
    tr_vals = train_df[num_cols].to_numpy()
    te_vals = test_df[num_cols].to_numpy()
    tr_vals[~np.isfinite(tr_vals)] = np.nan
    te_vals[~np.isfinite(te_vals)] = np.nan
    train_df[num_cols] = tr_vals
    test_df[num_cols]  = te_vals

    # 3) Clip percent-like fields to sensible bounds (do it raw, not scaled)
    num_only = train_df.drop(columns=["battle_id","player_won"], errors="ignore").select_dtypes(include=[np.number])
    percent_like = [x for x in num_only.columns if ("hp" in x.lower()) or ("auc" in x.lower())]
    for c in percent_like:
        if c in train_df.columns:
            train_df[c] = train_df[c].clip(lower=0, upper=100)
            test_df[c]  = test_df[c].clip(lower=0, upper=100)

    # 4) Flag near-constants (≥99% same value) — info only
    near_const = [c for c in num_only.columns if (num_only[c].nunique(dropna=True) / max(1, len(num_only)) < 0.01)]
    print(f"[Sanity] Near-constant features (not dropping yet): {len(near_const)}")

    # === 2.x Custom predictive features (safe: no NaN, no div-by-zero) ===

    EPS = 1e-6
    REPLACE_EXISTING = True  # set to False to skip creation if a feature name already exists

    def _pick_first(df: pd.DataFrame, candidates, default_value=0.0):
        """Return the first existing column from candidates; else a float32 Series filled with default_value."""
        for c in candidates:
            if c in df.columns:
                return df[c].astype("float32")
        return pd.Series(default_value, index=df.index, dtype="float32")

    def _safe_div(a: pd.Series, b: pd.Series):
        """Elementwise safe division a/(b+EPS) with finite output."""
        out = a.astype("float32") / (b.astype("float32") + EPS)
        out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0).astype("float32")
        return out

    def _ensure_float32(s: pd.Series):
        return s.astype("float32").replace([np.inf, -np.inf], 0.0).fillna(0.0)

    def _normalize_acc(s: pd.Series):
        """If accuracy looks like [0..100], scale to [0..1]."""
        s = _ensure_float32(s)
        if len(s):
            maxv = float(np.nanmax(s.values))
        else:
            maxv = 0.0
        if maxv > 1.5:  # heuristically assume it's a percentage
            s = s / 100.0
        return s.clip(0.0, 1.0)

    def _add_feature_pair(train_df, test_df, name, train_series, test_series):
        """Attach float32 features to both train and test with final sanitation."""
        if (not REPLACE_EXISTING) and (name in train_df.columns or name in test_df.columns):
            return
        train_df[name] = _ensure_float32(train_series)
        test_df[name]  = _ensure_float32(test_series)

    # --- Robust base columns (try multiple candidates, fall back to zeros) ---

    # Attack / Defense (means)
    atk_p1 = _pick_first(train_df, ["atk_p1_mean","atk_p1","atk_p1_full"], 0.0)
    atk_p2 = _pick_first(train_df, ["atk_p2_mean","atk_p2","atk_p2_full"], 0.0)
    def_p1 = _pick_first(train_df, ["def_p1_mean","def_p1","def_p1_full"], 0.0)
    def_p2 = _pick_first(train_df, ["def_p2_mean","def_p2","def_p2_full"], 0.0)

    atk_p1_te = _pick_first(test_df, ["atk_p1_mean","atk_p1","atk_p1_full"], 0.0)
    atk_p2_te = _pick_first(test_df, ["atk_p2_mean","atk_p2","atk_p2_full"], 0.0)
    def_p1_te = _pick_first(test_df, ["def_p1_mean","def_p1","def_p1_full"], 0.0)
    def_p2_te = _pick_first(test_df, ["def_p2_mean","def_p2","def_p2_full"], 0.0)

    # Special Attack / Defense (means)
    sp_atk_p1 = _pick_first(train_df, ["sp_atk_p1_mean","spatk_p1_mean","spa_p1_mean","sp_atk_p1"], 0.0)
    sp_atk_p2 = _pick_first(train_df, ["sp_atk_p2_mean","spatk_p2_mean","spa_p2_mean","sp_atk_p2"], 0.0)
    sp_def_p1 = _pick_first(train_df, ["sp_def_p1_mean","spdef_p1_mean","spd_p1_mean_def","sp_def_p1"], 0.0)
    sp_def_p2 = _pick_first(train_df, ["sp_def_p2_mean","spdef_p2_mean","spd_p2_mean_def","sp_def_p2"], 0.0)

    sp_atk_p1_te = _pick_first(test_df, ["sp_atk_p1_mean","spatk_p1_mean","spa_p1_mean","sp_atk_p1"], 0.0)
    sp_atk_p2_te = _pick_first(test_df, ["sp_atk_p2_mean","spatk_p2_mean","spa_p2_mean","sp_atk_p2"], 0.0)
    sp_def_p1_te = _pick_first(test_df, ["sp_def_p1_mean","spdef_p1_mean","spd_p1_mean_def","sp_def_p1"], 0.0)
    sp_def_p2_te = _pick_first(test_df, ["sp_def_p2_mean","spdef_p2_mean","spd_p2_mean_def","sp_def_p2"], 0.0)

    # Speed (means)
    spd_p1 = _pick_first(train_df, ["spd_p1_mean","speed_p1_mean","spd_p1"], 0.0)
    spd_p2 = _pick_first(train_df, ["spd_p2_mean","speed_p2_mean","spd_p2"], 0.0)
    spd_p1_te = _pick_first(test_df,  ["spd_p1_mean","speed_p1_mean","spd_p1"], 0.0)
    spd_p2_te = _pick_first(test_df,  ["spd_p2_mean","speed_p2_mean","spd_p2"], 0.0)

    # HP current / max
    hp1_cur = _pick_first(train_df, ["hp_p1_remain","hp_p1_curr","hp_p1"], 0.0)
    hp2_cur = _pick_first(train_df, ["hp_p2_remain","hp_p2_curr","hp_p2"], 0.0)
    hp1_max = _pick_first(train_df, ["hp_p1_max","hp_p1_base","hp_p1_total"], 1.0)
    hp2_max = _pick_first(train_df, ["hp_p2_max","hp_p2_base","hp_p2_total"], 1.0)

    hp1_cur_te = _pick_first(test_df, ["hp_p1_remain","hp_p1_curr","hp_p1"], 0.0)
    hp2_cur_te = _pick_first(test_df, ["hp_p2_remain","hp_p2_curr","hp_p2"], 0.0)
    hp1_max_te = _pick_first(test_df, ["hp_p1_max","hp_p1_base","hp_p1_total"], 1.0)
    hp2_max_te = _pick_first(test_df, ["hp_p2_max","hp_p2_base","hp_p2_total"], 1.0)

    # Move power / accuracy
    pwr_p1 = _pick_first(train_df, ["mv_p1_power_mean_full","mv_p1_power_mean","mv_power_p1_mean"], 0.0)
    pwr_p2 = _pick_first(train_df, ["mv_p2_power_mean_full","mv_p2_power_mean","mv_power_p2_mean"], 0.0)
    acc_p1 = _normalize_acc(_pick_first(train_df, ["mv_p1_acc_mean_full","mv_p1_acc_mean","mv_acc_p1_mean"], 0.0))
    acc_p2 = _normalize_acc(_pick_first(train_df, ["mv_p2_acc_mean_full","mv_p2_acc_mean","mv_acc_p2_mean"], 0.0))

    pwr_p1_te = _pick_first(test_df, ["mv_p1_power_mean_full","mv_p1_power_mean","mv_power_p1_mean"], 0.0)
    pwr_p2_te = _pick_first(test_df, ["mv_p2_power_mean_full","mv_p2_power_mean","mv_power_p2_mean"], 0.0)
    acc_p1_te = _normalize_acc(_pick_first(test_df, ["mv_p1_acc_mean_full","mv_p1_acc_mean","mv_acc_p1_mean"], 0.0))
    acc_p2_te = _normalize_acc(_pick_first(test_df, ["mv_p2_acc_mean_full","mv_p2_acc_mean","mv_acc_p2_mean"], 0.0))

    # Move type counts (STATUS / PHYSICAL / SPECIAL) — safe fallbacks
    st_p1 = _pick_first(train_df, ["mv_p1_count_STATUS_full","mv_p1_count_STATUS","status_moves_p1"], 0.0)
    ph_p1 = _pick_first(train_df, ["mv_p1_count_PHYSICAL_full","mv_p1_count_PHYSICAL","physical_moves_p1"], 0.0)
    sp_p1 = _pick_first(train_df, ["mv_p1_count_SPECIAL_full","mv_p1_count_SPECIAL","special_moves_p1"], 0.0)
    st_p2 = _pick_first(train_df, ["mv_p2_count_STATUS_full","mv_p2_count_STATUS","status_moves_p2"], 0.0)
    ph_p2 = _pick_first(train_df, ["mv_p2_count_PHYSICAL_full","mv_p2_count_PHYSICAL","physical_moves_p2"], 0.0)
    sp_p2 = _pick_first(train_df, ["mv_p2_count_SPECIAL_full","mv_p2_count_SPECIAL","special_moves_p2"], 0.0)

    st_p1_te = _pick_first(test_df, ["mv_p1_count_STATUS_full","mv_p1_count_STATUS","status_moves_p1"], 0.0)
    ph_p1_te = _pick_first(test_df, ["mv_p1_count_PHYSICAL_full","mv_p1_count_PHYSICAL","physical_moves_p1"], 0.0)
    sp_p1_te = _pick_first(test_df, ["mv_p1_count_SPECIAL_full","mv_p1_count_SPECIAL","special_moves_p1"], 0.0)
    st_p2_te = _pick_first(test_df, ["mv_p2_count_STATUS_full","mv_p2_count_STATUS","status_moves_p2"], 0.0)
    ph_p2_te = _pick_first(test_df, ["mv_p2_count_PHYSICAL_full","mv_p2_count_PHYSICAL","physical_moves_p2"], 0.0)
    sp_p2_te = _pick_first(test_df, ["mv_p2_count_SPECIAL_full","mv_p2_count_SPECIAL","special_moves_p2"], 0.0)

    # ===============================
    # 10 SAFE, HIGH-SIGNAL FEATURES
    # ===============================

    # 1) atk_def_ratio: P1 attack vs P2 defense
    _add_feature_pair(
        train_df, test_df, "atk_def_ratio",
        _safe_div(atk_p1, def_p2),
        _safe_div(atk_p1_te, def_p2_te)
    )

    # 2) spd_gap: P1 speed minus P2 speed
    _add_feature_pair(
        train_df, test_df, "spd_gap",
        (spd_p1 - spd_p2),
        (spd_p1_te - spd_p2_te)
    )

    # 3) hp_ratio: P1 current HP vs P2 current HP
    _add_feature_pair(
        train_df, test_df, "hp_ratio",
        _safe_div(hp1_cur, hp2_cur),
        _safe_div(hp1_cur_te, hp2_cur_te)
    )

    # 4) survival_score: (P1 HP%) - (P2 HP%)
    _add_feature_pair(
        train_df, test_df, "survival_score",
        _safe_div(hp1_cur, hp1_max) - _safe_div(hp2_cur, hp2_max),
        _safe_div(hp1_cur_te, hp1_max_te) - _safe_div(hp2_cur_te, hp2_max_te)
    )

    # 5) momentum_index: (atk*spd)_P1 / (atk*spd)_P2
    _add_feature_pair(
        train_df, test_df, "momentum_index",
        _safe_div(atk_p1 * spd_p1, atk_p2 * spd_p2),
        _safe_div(atk_p1_te * spd_p1_te, atk_p2_te * spd_p2_te)
    )

    # 6) power_acc_gap: (avg power weighted by acc) P1 - P2
    pwa_p1 = _ensure_float32(pwr_p1 * acc_p1)
    pwa_p2 = _ensure_float32(pwr_p2 * acc_p2)
    pwa_p1_te = _ensure_float32(pwr_p1_te * acc_p1_te)
    pwa_p2_te = _ensure_float32(pwr_p2_te * acc_p2_te)
    _add_feature_pair(
        train_df, test_df, "power_acc_gap",
        (pwa_p1 - pwa_p2),
        (pwa_p1_te - pwa_p2_te)
    )

    # 7) offensive_balance: (atk + sp_atk) P1 / P2
    _add_feature_pair(
        train_df, test_df, "offensive_balance",
        _safe_div(atk_p1 + sp_atk_p1, atk_p2 + sp_atk_p2),
        _safe_div(atk_p1_te + sp_atk_p1_te, atk_p2_te + sp_atk_p2_te)
    )

    # 8) defensive_efficiency: (def + sp_def) P1 / P2
    _add_feature_pair(
        train_df, test_df, "defensive_efficiency",
        _safe_div(def_p1 + sp_def_p1, def_p2 + sp_def_p2),
        _safe_div(def_p1_te + sp_def_p1_te, def_p2_te + sp_def_p2_te)
    )

    # 9) status_influence: share STATUS moves P1 - P2
    tot_p1 = _ensure_float32(st_p1 + ph_p1 + sp_p1).replace(0.0, 1.0)
    tot_p2 = _ensure_float32(st_p2 + ph_p2 + sp_p2).replace(0.0, 1.0)
    tot_p1_te = _ensure_float32(st_p1_te + ph_p1_te + sp_p1_te).replace(0.0, 1.0)
    tot_p2_te = _ensure_float32(st_p2_te + ph_p2_te + sp_p2_te).replace(0.0, 1.0)

    status_share_p1 = _safe_div(st_p1, tot_p1)
    status_share_p2 = _safe_div(st_p2, tot_p2)
    status_share_p1_te = _safe_div(st_p1_te, tot_p1_te)
    status_share_p2_te = _safe_div(st_p2_te, tot_p2_te)

    _add_feature_pair(
        train_df, test_df, "status_influence",
        (status_share_p1 - status_share_p2),
        (status_share_p1_te - status_share_p2_te)
    )

    # 10) speed_ratio: P1 speed / P2 speed
    _add_feature_pair(
        train_df, test_df, "speed_ratio",
        _safe_div(spd_p1, spd_p2),
        _safe_div(spd_p1_te, spd_p2_te)
    )

    # --- Quick validation: no NaN/Inf and report how many were added ---
    new_cols = [
        "atk_def_ratio","spd_gap","hp_ratio","survival_score","momentum_index",
        "power_acc_gap","offensive_balance","defensive_efficiency","status_influence","speed_ratio"
    ]
    bad_train = train_df[new_cols].isna().sum().sum() + np.isinf(train_df[new_cols].to_numpy()).sum()
    bad_test  = test_df[new_cols].isna().sum().sum()  + np.isinf(test_df[new_cols].to_numpy()).sum()
    print(f"[FeatureEng] Added {len(new_cols)} engineered features. Bad values -> train: {bad_train}, test: {bad_test}")

    print("\nPreview (raw):")
    try:
        from IPython.display import display
        display(train_df_raw.head())
    except:
        print(train_df_raw.head())

    print("\nPrepared (unscaled, clean types):")
    try:
        from IPython.display import display
        display(train_df.head())
    except:
        print(train_df.head())

    return train_df, test_df, train_df_raw, test_df_raw, POKEMON_STATS, POKEMON_HP_STATS, pokemon_avg_damage